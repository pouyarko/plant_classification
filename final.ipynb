{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_dir = \"Runs/\"\n",
    "model_name = \"Hand_written_CNN_model\"\n",
    "model_dir = runs_dir + model_name + \"/\"\n",
    "last_epoch = -1\n",
    "Epochs = 30\n",
    "train_batch_size = 32\n",
    "test_batch_size = 16\n",
    "img_width, img_height = 256, 256 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data in 3 parts(train, validation, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to dataset\n",
    "dataset_path = './plantvillage dataset/color'\n",
    "\n",
    "# Path to splited output\n",
    "output_path = './split1'\n",
    "\n",
    "classes = os.listdir(dataset_path)\n",
    "\n",
    "# Parameters\n",
    "test_size = 0.05  # 5% for test set\n",
    "val_size = 0.12  # 12% of the remaining 80% for the validation set\n",
    "\n",
    "# Function to copy files\n",
    "def copy_files(files, source, dest):\n",
    "    os.makedirs(dest, exist_ok=True)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(source, file)\n",
    "        if os.path.isfile(file_path):  # Ensure it's a file, not a directory\n",
    "            shutil.copy(file_path, dest)\n",
    "\n",
    "# Splitting and copying the dataset\n",
    "for class_name in tqdm(classes):\n",
    "    class_path = os.path.join(dataset_path, class_name)\n",
    "    images = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
    "\n",
    "    # Splitting\n",
    "    train_val, test = train_test_split(images, test_size=test_size, random_state=42)\n",
    "    train, val = train_test_split(train_val, test_size=val_size, random_state=42)\n",
    "\n",
    "    # Copying files\n",
    "    for dataset_type, dataset_files in zip(['train', 'val', 'test'], [train, val, test]):\n",
    "        dest_path = os.path.join(output_path, dataset_type, class_name)\n",
    "        copy_files(dataset_files, class_path, dest_path)\n",
    "\n",
    "print(\"Dataset successfully split into training, validation, and test sets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing number of data points in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = './split/train'\n",
    "\n",
    "classes = sorted(os.listdir(ds_path))\n",
    "num_classes = len(classes)\n",
    "l =[]\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    clas = classes[i]\n",
    "    l.append((len(os.listdir(f\"{ds_path}/{clas}\")),clas))\n",
    "\n",
    "l.sort(key=lambda x: x[0])\n",
    "\n",
    "for i in l:\n",
    "     print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and Compiling a simple CNN Classifier with dialation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACTOR = 1\n",
    "x_inp = tf.keras.layers.Input(shape=(img_width, img_height, 3))\n",
    "padding_layer = tf.keras.layers.ZeroPadding2D(padding=(2, 2))(x_inp) # 184\n",
    "c1 = tf.keras.layers.Conv2D(int(24 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(padding_layer) # 184\n",
    "c_dial_1 = tf.keras.layers.Conv2D(int(8 * FACTOR), (3, 3), dilation_rate=1, padding=\"same\", activation=tf.nn.leaky_relu)(padding_layer)\n",
    "c1_out = tf.keras.layers.Concatenate(axis = 3)([c1, c_dial_1])\n",
    "c2 = tf.keras.layers.Conv2D(int(12 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(c1_out) # 184\n",
    "c_dial_2 = tf.keras.layers.Conv2D(int(4 * FACTOR), (3, 3), dilation_rate=1, padding=\"same\", activation=tf.nn.leaky_relu)(c1_out)\n",
    "c2_out = tf.keras.layers.Concatenate(axis = 3)([c2, c_dial_2])\n",
    "c3 = tf.keras.layers.Conv2D(int(12 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(c2_out) # 184\n",
    "c_dial_3 = tf.keras.layers.Conv2D(int(4 * FACTOR), (3, 3), dilation_rate=1, padding=\"same\", activation=tf.nn.leaky_relu)(c2_out)\n",
    "c3_out = tf.keras.layers.Concatenate(axis = 3)([c3, c_dial_3])\n",
    "c4 = tf.keras.layers.Conv2D(int(12 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(c3_out) # 184\n",
    "c_dial_4 = tf.keras.layers.Conv2D(int(4 * FACTOR), (3, 3), dilation_rate=1, padding=\"same\", activation=tf.nn.leaky_relu)(c3_out)\n",
    "c4_out = tf.keras.layers.Concatenate(axis = 3)([c4, c_dial_4])\n",
    "c5 = tf.keras.layers.Conv2D(int(12 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(c4_out) # 184\n",
    "c_dial_5 = tf.keras.layers.Conv2D(int(4 * FACTOR), (3, 3), dilation_rate=1, padding=\"same\", activation=tf.nn.leaky_relu)(c4_out)\n",
    "c5_out = tf.keras.layers.Concatenate(axis = 3)([c5, c_dial_5])\n",
    "p1 = tf.keras.layers.MaxPooling2D()(c5_out)\n",
    "c5 = tf.keras.layers.Conv2D(int(16 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(p1) # 92\n",
    "c6 = tf.keras.layers.Conv2D(int(16 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(c5) # 92\n",
    "p2 = tf.keras.layers.MaxPooling2D()(c6)\n",
    "c7 = tf.keras.layers.Conv2D(int(32 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(p2) # 46\n",
    "c8 = tf.keras.layers.Conv2D(int(32 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(c7) # 46\n",
    "p3 = tf.keras.layers.MaxPooling2D()(c8)\n",
    "c9 = tf.keras.layers.Conv2D(int(64 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(p3) # 23\n",
    "c10 = tf.keras.layers.Conv2D(int(64 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(c9) # 23\n",
    "p4 = tf.keras.layers.MaxPooling2D()(c10)\n",
    "c11 = tf.keras.layers.Conv2D(int(128 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(p4) # 12\n",
    "c12 = tf.keras.layers.Conv2D(int(128 * FACTOR), (3, 3), padding=\"same\", activation=tf.nn.leaky_relu)(c11) # 12\n",
    "p7 = tf.keras.layers.GlobalMaxPooling2D()(c12)\n",
    "\n",
    "d1 = tf.keras.layers.Dense(64, activation=tf.nn.leaky_relu)(p7)\n",
    "d2 = tf.keras.layers.Dense(32, activation=tf.nn.leaky_relu)(d1)\n",
    "out_layer = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(d2)\n",
    "model = tf.keras.models.Model(inputs=[x_inp], outputs=[out_layer])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.0004), metrics=['accuracy'],loss=tf.keras.losses.categorical_crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a data generator which yields batch size of data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datagen(ds_path,b):\n",
    "    \n",
    "    classes = sorted(os.listdir(ds_path))\n",
    "    num_classes = len(classes)\n",
    "    l =[]\n",
    "    names = [sorted(os.listdir(f'{ds_path}/{i}')) for i in classes]\n",
    "    for i in range(len(classes)):\n",
    "        clas = classes[i]\n",
    "        l.append((len(os.listdir(f\"{ds_path}/{clas}\")),clas))\n",
    "    remaining = [x[0] for x in l]\n",
    "    while 1:\n",
    "        for i in range(num_classes):\n",
    "            images = np.zeros((b,256,256,3))\n",
    "            labels = np.zeros((b,num_classes))\n",
    "            if remaining[i]>b:\n",
    "                for j in range(b):\n",
    "                    images[j] = plt.imread(f\"{ds_path}/{classes[i]}/{names[i][j+l[i][0]-remaining[i]]}\")[:,:,0:3]\n",
    "                    labels[j][i] = 1 \n",
    "                remaining[i] -= b\n",
    "                \n",
    "            else:\n",
    "                remaining[i] = l[i][0]\n",
    "                for j in range(b):\n",
    "                    images[j] = plt.imread(f\"{ds_path}/{classes[i]}/{names[i][j+l[i][0]-remaining[i]]}\")[:,:,0:3]\n",
    "                    labels[j][i] = 1\n",
    "                remaining[i] -= b\n",
    "        \n",
    "            yield images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating generator objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datagen('./split/train/',train_batch_size)\n",
    "val = datagen('./split/val/',train_batch_size)\n",
    "test = datagen('./split/test/',test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(last_epoch + 1, Epochs):\n",
    "    print(\"Epoch\", e + 1, \"/\", Epochs, \":\")\n",
    "\n",
    "    model.fit(x = train, batch_size = train_batch_size ,validation_data= val, epochs = 1,steps_per_epoch=1000 , shuffle = True, validation_steps=20)\n",
    "\n",
    "    model.save(model_dir + model_name + \"_\" + str(e) + \".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained =  tf.keras.models.load_model(\"./Runs/Hand_written_CNN_model/Hand_written_CNN_model_6.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained.evaluate(x= test, batch_size=train_batch_size, steps=54)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a pretrained model fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)  \n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)  \n",
    "\n",
    "model_res = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "model_res.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_res.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
